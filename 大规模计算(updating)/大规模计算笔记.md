[TOC]

# Introduction

两个项目

- 50 pts Algorithm programming
  concurrent programming projects with MPI, OpenMP, CUDA and Big Data framework
  MPI/Spark + GPU 热传导方程计算
- 50 pts Integration programming 计算广告ctr

<img src="assets/284df631ce66090cdd4765cdc5e682a.jpg" alt="284df631ce66090cdd4765cdc5e682a" style="zoom:25%;" />

MPI
OpenMP
Pthread
CUDA
Hadoop&Spark

# Tools

## python

- [x] MPI4PY
- [ ] Numba
- [ ] PyCUDA
- [ ] PySpark

## platform

高性能计算

# HPC

## Conceptual Model of a LSC(large scale Data or Modules)'s program

divide & conquer

**decomposition and collaboration**

ode, pde, grid, 有限元分析

## Sequential implementation with Python

### Configure the parameters

#### Grid

- with inital values
- boundary conditions

#### Termination condition

- Iteration number or Epsilon

### Iterative updating

use "Termination condition" to control the updating of the internal vertices

### Visualize the dynamics

matplotlib

## Ideas to convert Sequential to Parallel

### DAOM

- **Decomposition** of computation in tasks
- **Assignment** of tasks to processes
- **Orchestration** of data access, comm, synch
- **Mapping** processes to processors

### PCAM

- Partitioning
- Communication
- Agglomeration or aggregation
- Mapping

# LSC(Large Scale Computing System)

# Mpi4py

## 点到点通讯

### 传递通用的python对象(阻塞方式)

```python
# p2p_blocking.py

from mpi4py import MPI


comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = {'a': 7, 'b': 3.14}
    print 'process %d sends %s' % (rank, data)
    comm.send(data, dest=1, tag=11)
elif rank == 1:
    data = comm.recv(source=0, tag=11)
    print 'process %d receives %s' % (rank, data)
```

### 传递通用的python对象(非阻塞方式)

```python
# p2p_non_blocking.py

from mpi4py import MPI


comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = {'a': 7, 'b': 3.14}
    print 'process %d sends %s' % (rank, data)
    req = comm.isend(data, dest=1, tag=11)
    req.wait()
elif rank == 1:
    req = comm.irecv(source=0, tag=11)
    data = req.wait()
    print 'process %d receives %s' % (rank, data)
```

### 传递numpy数组

可以用一种更为高效的方式直接进行传递，而不需要经过 pickle 系列化和恢复。以这种方式传递数据需要使用通信子对象的以大写字母开头的方法，如 Send()，Recv()，Bcast()，Scatter()，Gather() 等

```python
# p2p_numpy_array.py

import numpy
from mpi4py import MPI


comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# passing MPI datatypes explicitly
if rank == 0:
    data = numpy.arange(10, dtype='i')
    print 'process %d sends %s' % (rank, data)
    comm.Send([data, MPI.INT], dest=1, tag=77)
elif rank == 1:
    data = numpy.empty(10, dtype='i')
    comm.Recv([data, MPI.INT], source=0, tag=77)
    print 'process %d receives %s' % (rank, data)

# automatic MPI datatype discovery
if rank == 0:
    data = numpy.arange(10, dtype=numpy.float64)
    print 'process %d sends %s' % (rank, data)
    comm.Send(data, dest=1, tag=13)
elif rank == 1:
    data = numpy.empty(10, dtype=numpy.float64)
    comm.Recv(data, source=0, tag=13)
    print 'process %d receives %s' % (rank, data)
```

在代码几乎一样的情况下，以大写字母开头的 Send()/Recv() 方法对 numpy 数组的传递效率要高的多，因此在涉及 numpy 数组的并行操作时，应尽量选择以大写字母开头的通信方法。

## 集合通讯

### 广播

广播操作将根进程的数据复制到同组内其他所有进程中。

#### 广播通用的Python对象

```python
# bcast.py

from mpi4py import MPI


comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = {'key1' : [7, 2.72, 2+3j],
            'key2' : ( 'abc', 'xyz')}
    print 'before broadcasting: process %d has %s' % (rank, data)
else:
    data = None
    print 'before broadcasting: process %d has %s' % (rank, data)

data = comm.bcast(data, root=0)
print 'after broadcasting: process %d has %s' % (rank, data)
```

#### 广播numpy数组

```python
# Bcast.py

import numpy as np
from mpi4py import MPI


comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = np.arange(10, dtype='i')
    print 'before broadcasting: process %d has %s' % (rank, data)
else:
    data = np.zeros(10, dtype='i')
    print 'before broadcasting: process %d has %s' % (rank, data)

comm.Bcast(data, root=0)

print 'after broadcasting: process %d has %s' % (rank, data)
```

### 发散

发散操作从组内的根进程分别向组内其它进程散发不同的消息。

#### 发散通用的Python对象

```python
# scatter.py

from mpi4py import MPI


comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

if rank == 0:
    data = [ (i + 1)**2 for i in range(size) ]
    print 'before scattering: process %d has %s' % (rank, data)
else:
    data = None
    print 'before scattering: process %d has %s' % (rank, data)

data = comm.scatter(data, root=0)
print 'after scattering: process %d has %s' % (rank, data)
```

#### 发散numpy数组

```python
# Scatter.py

import numpy as np
from mpi4py import MPI


comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

sendbuf = None
if rank == 0:
    sendbuf = np.empty([size, 10], dtype='i')
    sendbuf.T[:, :] = range(size)
print 'before scattering: process %d has %s' % (rank, sendbuf)

recvbuf = np.empty(10, dtype='i')
comm.Scatter(sendbuf, recvbuf, root=0)
print 'after scattering: process %d has %s' % (rank, recvbuf)
```

### 收集

收集操作是发散的逆操作，根进程从其它进程收集不同的消息依次放入自己的接收缓冲区内。

#### 收集通用的Python对象

```python
# gather.py

from mpi4py import MPI


comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

data = (rank + 1)**2
print 'before gathering: process %d has %s' % (rank, data)

data = comm.gather(data, root=0)
print 'after scattering: process %d has %s' % (rank, data)
```

#### 收集numpy数组

```python
# Gather.py

import numpy as np
from mpi4py import MPI


comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

sendbuf = np.zeros(10, dtype='i') + rank
print 'before gathering: process %d has %s' % (rank, sendbuf)

recvbuf = None
if rank == 0:
    recvbuf = np.empty([size, 10], dtype='i')

comm.Gather(sendbuf, recvbuf, root=0)
print 'after gathering: process %d has %s' % (rank, recvbuf)
```